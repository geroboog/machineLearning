{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "A1_ReportDraft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bRwQ9JwiSq3",
        "colab_type": "text"
      },
      "source": [
        "# Draft and Experiment Area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-p-eY9diSq5",
        "colab_type": "text"
      },
      "source": [
        "1. First impression\n",
        "    * What is my chosen paper to read?\n",
        "    * What type of the main contribution the paper has made?\n",
        "        - A theory or proposition (revealing something, from unknown to known)\n",
        "        - A method or algorithm (inventing a technique, from undoable to doable)\n",
        "\n",
        "    * _Before_ reading the main body of the paper, write down your first impression  obtained from its abstract and short introduction.\n",
        "    * Why does the paper attract you, such as, How it surprised you? Why do you think it addresses an important topic that will be helpful in your future study of machine learning?\n",
        "    \n",
        "2. Read the paper abstract and introduction, list here all the notions that you don't know the precise meaning. If you think you have completed your list,  compare the list with people around you who have chosen the same or a similar paper.\n",
        "\n",
        "3. (During the next 7 days) Re-consider the central problem of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpPHMAXbiSq6",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Gradient-Based Learning Applied to Document Recognition\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP3uHEAliSq6",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The review report is about giving an overview of the research paper- \"Gradient-Based Learning Applied to Document Recognition\" which designed an innovating and successful multi-module architecture called Graph Transform Network for pattern recognition and also evaluate the performance of the convolutional nerual network for identifying handwritten characters. However, even many innovations and improvements were achieved in this research, there are still some limitaion of the research. In this review paper, firstly, the summarized content of the research will be described in the content section. Secondly, the innovation of the research will be explained detailly after the content section. Thridly, from the evaluation part of original paper, the technical quality will be talked about. Fourthly, this report will arrive to the application of the new technology. Finally, the last section will analyze how the author orgnized the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afQWhuOKiSq7",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z5hux4SiSq7",
        "colab_type": "text"
      },
      "source": [
        "In the past, most pattern recognition systems are built using a combination of automatic learning techniques and hand-crafted algorithms. This literature introduces two remarkable graph transform networks(GTN) with multi-modules for segmenting, classifying, and resequencing the handwritten characters string, which elistimates the manual work and increase the accuracy for the pattern recognition. The first GTN is combined by 4 modules the Segmentation Transformer, Character Recognition Transformer(CRT), Composition Transformer, and Beam Search Transformer. Specifically, the segmentation transformer will uses Heuristic Over-segmentaion algorithm to devide the whole picture to a set of arcs(segments) as the input of CRT. And then the CRT which is the most important part of the research is consitubed by the CNN trained with Gradient-based learning and back-propagation method, such as the LeNet-5 with 7 layers as the simplest CNN example described detailly in the research. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Firstly, the algorithm for extracting feature image from the original image is introduced step by step, which can generate the feature images for training, testing and verifying the model. Besides, the distinguished neural network - Convolutional Neural Network is designed by the authors in this paper for optimizing the pattern recognition rate.\n",
        "\n",
        "A combination of three factors have changed this vision over the last decade. First, the availability of low-cost machines with fast arithmetic units allows to rely more on brute-force ^numericaP, methods than on algorithmic refinements. Second, the availability of large databases for problems with a large market and wide interest, such as handwriting recognition, has enabled designers to rely more on real data and less on hand-crafted feature extraction to build recognition systems. The third and very important factor is the availability of powerful machine learning techniques that can handle high-dimensional inputs and can generate intricate decision functions when fed with these large data sets. It can be argued that the recent progress in the accuracy of speech and handwriting recognition systems can be attributed in large part to an increased reliance on learning techniques and large training data sets. As evidence to this fact, a large proportion of modern commercial OCR systems use some form of multi-layer Neural Network trained with back-propagation.\n",
        "\n",
        "\n",
        "Discriminative and non-discriminative gradient-based techniques for training a recognizer at the word level without requiring manual segmentation and labeling are presented in Section VI.\n",
        "\n",
        "\n",
        "Section VII presents the promising Space Displacement Neural Network approach that eliminates the need for segmentation heuristics by scanning a recognizer at all possible locations on the input.\n",
        "\n",
        "\n",
        "In section VIII, it is shown that trainable Graph Transformer Networks can be formulated as multiple generalized transductions, based on a general graph composition algorithm. The connections between GTNs and Hidden Markov Models, commonly used in speech recognition is also treated.\n",
        "\n",
        "Section IX describes a globally trained GTN system for recognizing handwriting entered in a pen computer. This problem is known as \"on-line‚Äù handwriting recognition, since the machine must produce immediate feedback as the user writes. The core of the system is a Convolutional Neural Network. The results clearly demonstrate the advantages of training a recognizer at the word level, rather than training it on pre-segmented, hand-labeled, isolated characters. Section X describes a complete GTN-based system for reading handwritten and machine-printed bank checks. The core of the system is the Convolutional Neural Network called LeNet-5 described in Section II.\n",
        "\n",
        "\n",
        "on-line  handwriting recognition\n",
        "\n",
        "\n",
        "  Gradient Back\u0003Propagation local minima do not seem to be a problem for multi-layer neural networks is somewhat of a theoretical mystery.non-linear gradient-based Learning techniques such as Boltzmann machines.\n",
        "  \n",
        "  The third event was the demonstration that the back-propagation procedure applied to multi-layer neural networks with sigmoidal units can solve complicated learning tasks.\n",
        "  \n",
        "  \n",
        "  The basic idea of back-propagation is that gradients can be computed efficiently by propagation from the output to the input. This idea was described in the control theory literature of the early sixties [16], but its application to machine learning was not generally realized then. Interestingly, the early derivations of back-propagation in the context of neural network learning did not use gradients, but ^virtual targets^ for units in intermediate layers [17], [18], or minimal disturbance arguments [19].\n",
        "  \n",
        "  \n",
        "  In this case, each module, called a Graph Transformer, takes one or more graphs as input, and produces a graph as output. Networks of such modules are called Graph Transformer Networks (GTN). Sections IV, VI and VIII develop the concept of GTNs, and show that Gradient-Based Learning can be used to train all the parameters in all the modules so as to mini\u001fmize a global loss function. It may seem paradoxical that gradients can be computed when the state information is represented by essentially discrete objects such as graphs, but that difficulty can be circumvented, as shown later.\n",
        "  \n",
        "  LeNet-5 including 7 layers for classifing single character.\n",
        "  \n",
        "  Multi\u0003Module Systems and Graph Transformer Networks       An Object\u0003Oriented Approach by each modules have the fprop(forward propagation) function, which is considered as a feed-forward network.\n",
        "\n",
        "\n",
        "Graph Transformer Networks by modules-Graph Transformer. Each module is designed for different task.Multiple Object Recognition: Heuristic Over-segmentation, Character Recognition.\n",
        "\n",
        "The recognition transformer Trec takes the segmentation graph Gseg as input\u0004 and applies the recognizer for single characters to the images associated with each of the arcs in the segmentation graph.\n",
        "\n",
        "sequence recognition systems training recurrent Neural networks is too hard with gradient-based techniques, therefore this paper suggest a simply method-the Viterbi algorithm\n",
        "\n",
        "\n",
        "Discriminative Viterbi Training GTN Architecture for a character string recognizer based on Heuristic Over-Segmentation. \n",
        "\n",
        "Multiple Object Recognition: Space Displacement Neural Network\n",
        "\n",
        "A Space Displacement Neural Network is a convolutional network that has been replicated over a wide input field\n",
        "In this paper  LeNet-5 SDNN is used for experimence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrV1o8LiSq8",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq8S26DliSq9",
        "colab_type": "text"
      },
      "source": [
        "The main message of this paper is that better pattern recognition systems can be built by relying more on automatic learning, and less on hand-designed heuristics.\n",
        "\n",
        "Using document understanding as a case study, we show that the traditional way of building recognition systems by manually integrating individually designed modules can be replaced by a unified and well-principled design paradigm, called Graph Transformer Networks^ that allows training all the modules to optimize a global performance criterion.\n",
        "\n",
        "Since the early days of pattern recognition it has been known that the variability and richness of natural data, be it speech, glyphs, or other types of patterns, make it almost impossible to build an accurate recognition system entirely by hand. Consequently, most pattern recognition systems are built using a combination of automatic learning techniques and hand-crafted algorithms.\n",
        "\n",
        "\n",
        "The feature extractor contains most of the prior knowledge and is rather specific to the task. It is also the focus of most of the design effort, because it is often entirely hand-crafted. The classifier, on the other hand, is often general-purpose and trainable. One of the main problems with this approach is that the recognition accuracy is largely determined by the ability of the designer to come up with an appropriate set of features. This turns out to be a daunting task which, unfortunately, must be redone for each new problem. A large amount of the pattern recognition literature is devoted to describing and comparing the relative\n",
        "\n",
        "\n",
        "\n",
        "Convolutional Neural Networks introduced in Section II are an example of specialized neural network architectures which incorporate knowledge about the invariances of 2D shapes by using local connection patterns, and by imposing constraints on the weights.\n",
        "\n",
        "\n",
        "One of the most difficult problems in handwriting recog\u001fnition, however, is not only to recognize individual charac\u001fters, but also to separate out characters from their neigh\u001fbors within the word or sentence, a process known as seg\u001fmentation. The technique for doing this that has become the \"standard‚Äù is called **Heuristic Over-Segmentation**.\n",
        "\n",
        "\n",
        "Section V explores various ways to ensure that the loss function is dif\u001fferentiable, and therefore lends itself to the use of Gradient\u001fBased Learning methods. Section V introduces the use of directed acyclic graphs whose arcs carry numerical infor\u001fmation as a way to represent the alternative hypotheses, and introduces the idea of GTN.\n",
        "\n",
        "\n",
        "The second solution described in Section VII is to elim\u001finate segmentation altogether. The idea is to sweep the recognizer over every possible location on the input image, and to rely on the ^character spotting‚Äù property of the rec\u001fognizer, i.e. its ability to correctly recognize a well-centered character in its input field, even in the presence of other characters besides it, while rejecting images containing no centered characters [26], [27]. The sequence of recognizer outputs obtained by sweeping the recognizer over the in\u001fput is then fed to a Graph Transformer Network that takes linguistic constraints into account and finally extracts the most likely interpretation. This GTN is somewhat similar to Hidden Markov Models (HMM), which makes the ap\u001fproach reminiscent of the classical speech recognition [28], [29]. While this technique would be quite expensive in the general case, the use of Convolutional Neural Networks makes it particularly attractive because it allows significant savings in computational cost.\n",
        "\n",
        "\n",
        "\n",
        "Convolutional Neural Networks-----reduce the need for hand\u0003crafted heuristics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nqCJa4wrB7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOqtiUu6iSq9",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGPw647viSq-",
        "colab_type": "text"
      },
      "source": [
        "The technology of the research is in high quality. Generally speaking,in the C part of the section I, the author references the Boltzmann machines*** to prove that the multi-layer neural network achieved by gradient-back propagation algorithm is feasible for classifying non-linear objects. Besides, in the section II, for comparing the performance of CNN with other classifying algorithm, the author experimented more than 11 algorithm like SVM, K-NN, PCA, Tangent Distance and LeNet-1 to classify the single character image to be character, and the LeNet-5 shows the best performance(0.95% errors) without the help of articially distorted examples(LeNet-5 0.8% errors) and boosting method(LeNet-4 0.7% errors). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z-F2o36iSq_",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LduLS9pIiSq_",
        "colab_type": "text"
      },
      "source": [
        "I find the proposal in the paper promising. ...\n",
        "\n",
        " This system is in commercial use in the NCR Corporation line of check recognition systems for the bank\u001fing industry. It is reading millions of checks per month in several banks across the United States."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVADqy9OiSrA",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oJfVdQLiSrB",
        "colab_type": "text"
      },
      "source": [
        "The overall strucutre is clear. I found reading is easy / difficult. The paper could have been more attractive if the authors had organised ... / provided ... \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFAaLVfViSrB",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[SHA48][1]: Author, Title, Info\n",
        "\n",
        "[1]:https://google.com"
      ]
    }
  ]
}